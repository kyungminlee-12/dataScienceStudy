{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 회귀 (Linear Regression)\n",
    "~~~\n",
    "선형 회귀: 변수 사이의 선형적인 관계를 모델링 한 것\n",
    "\n",
    "선형적이다: 직선적이다\n",
    "\n",
    "선형 회귀 모델은 주어진 데이터를 학습시켜서 가장 합리적인 직선을 찾아내는 것이다. 따라서 데이터가 3개 이상일 때 의미가 있다\n",
    "~~~\n",
    "\n",
    "**H(x) =Wx+b**\n",
    "\n",
    "하나의 일차 방정식을 이용해 직선을 표현, 가설을 수정해 나가면서 가장 합리적인 식을 찾아냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**즉 선형 회구란 주어진 데이터를 이용해 일차방정식을 수정해 나가는 것**\n",
    "~~~\n",
    "학습을 거쳐 가장 합리적인 선을 찾아내는 것\n",
    "학습을 많이 해도 '완벽한' 식을 찾아내지 못할 수 있다\n",
    "하지만 실제 사례에서는 근사값을 찾는 것 만으로도 충분할 때가 많다\n",
    "알파고도 결과적으로는 '근사값'을 가정하는 프로그램에 불과\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**비용 함수 (Cost Function)**\n",
    "\n",
    "비용: 가설이 얼마나 정확한 지 판단하는 기준\n",
    "~~~\n",
    "비용 계산하기 (직선과 점 사이의 거리)\n",
    "=> (예측 값-실제 값)**2 의 평균\n",
    "    현재의 W, b 값과 데이터를 이용하면 비용 함수를 구할 수 있다\n",
    "    비용 함수로 구한 비용이 적을 수록 좋다\n",
    "~~~\n",
    "\n",
    "**경사 하강 (Gradient Descent)**\n",
    "\n",
    "경사 하강을 이용해 합리적인 식 도출\n",
    "~~~\n",
    "H(x)=Wx로 식을 간단히 한다\n",
    "따라서 비용 함수는 (Wx-y)**2를 따른다\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우 경사 하강 라이브러리\n",
    "~~~\n",
    "우리는 직접 구현 할 필요 없이 원리만 이해해도 충분하다\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xData=[1, 2, 3, 4, 5, 6, 7]\n",
    "yData=[25000, 55000, 75000, 110000, 128000, 155000,180000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=tf.Variable(tf.random.uniform([1], -100, 100))  #W는 weight라는 의미, \n",
    "                                                  #random한 값으로 -100~100 들어감  \n",
    "b=tf.Variable(tf.random.uniform([1], -100, 100))    #tf.random.uniform으로 최신 버전에서는 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#틀 생성 과정\n",
    "tf.compat.v1.disable_eager_execution()     #tensorflow가 update됬나봄, 그래서 compat.v1.라는 코드를 추가해 전 ver code도 작동하게 함\n",
    "X=tf.compat.v1.placeholder(tf.float32)\n",
    "Y=tf.compat.v1.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=W+X+b\n",
    "cost=tf.reduce_mean(tf.square(H-Y))   #square라는 함수는 제곱 의미, reduce_mean: 평균값 구하기\n",
    "a=tf.Variable(0.01)\n",
    "optimizer=tf.compat.v1.train.GradientDescentOptimizer(a)  #GradientDescentOptimizer는 경사 하강 라으브러리\n",
    "train=optimizer.minimize(cost)      #비용 함수를 가장 적게 만드는 방향으로 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.compat.v1.global_variables_initializer()\n",
    "sess=tf.compat.v1.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 횟수와 기울기: 0 11817220000.0 [4107.9756] [4083.1772]\n",
      "학습 횟수와 기울기: 500 2638652000.0 [52010.35] [51985.55]\n",
      "학습 횟수와 기울기: 1000 2638652000.0 [52010.35] [51985.55]\n",
      "학습 횟수와 기울기: 1500 2638652000.0 [52010.35] [51985.55]\n",
      "학습 횟수와 기울기: 2000 2638652000.0 [52010.35] [51985.55]\n",
      "학습 횟수와 기울기: 2500 2638652000.0 [52010.35] [51985.55]\n",
      "학습 횟수와 기울기: 3000 2638652000.0 [52010.35] [51985.55]\n",
      "학습 횟수와 기울기: 3500 2638652000.0 [52010.35] [51985.55]\n",
      "학습 횟수와 기울기: 4000 2638652000.0 [52010.35] [51985.55]\n",
      "학습 횟수와 기울기: 4500 2638652000.0 [52010.35] [51985.55]\n",
      "학습 횟수와 기울기: 5000 2638652000.0 [52010.35] [51985.55]\n",
      "8시간 근무 예측 값: [104003.91]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5001):\n",
    "    sess.run(train, feed_dict={X: xData, Y: yData})    #실제로 학습 진행\n",
    "    \n",
    "    if i%500==0:\n",
    "        #현재까지 진행 된 학습의 횟수 출력, 기울기도 출력\n",
    "        print('학습 횟수와 기울기:',i, sess.run(cost, feed_dict={X: xData, Y: yData}), sess.run(W), sess.run(b))   \n",
    "\n",
    "#모든 학습이 끝난 후에 원하는 입력에 대한 결과를 출력\n",
    "print('8시간 근무 예측 값:',sess.run(H, feed_dict={X: [8]}))     #8 시간 일 했을 때의 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**다른 코드**\n",
    "~~~\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data=[1,2,3]\n",
    "y_data=[1,2,3]\n",
    "\n",
    "W=tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b=tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "hypothesis=W*x_data+b        #수식\n",
    "\n",
    "cost=tf.reduce_mean(tf.squater(hypothesis-y_ddata))\n",
    "\n",
    "rate=tf.Wariable(0.1)     #learning rate, alpha\n",
    "optimizer=tf.train.GradientDescentOptimizer(rate)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "init=tf.initialize_all_variables()\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step%20==0:\n",
    "        print('{:4} {} {} {}'.format(step, sess.run(cost), sess.run(W), sess.run(b)))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
